---
title: "Multidimensional Scaling"
author: "Jyotishka Datta"
date: "2020/01/24 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
  css: mystyle.css
lib_dir: libs
nature:
  highlightStyle: github
highlightLines: true
countIncrementalSlides: false
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
options(
  htmltools.dir.version = FALSE, # for blogdown
  width=80
)

# library(emo)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

## Idea of MDS 

 -  Represent high-dimensional point cloud in few (usually 2) dimensions keeping distances between points similar.
 -  Useful tool in visualizing any big data-set, specially for clustering purposes. 
 -  Is a popular exploratory tool. Used before any inferential procedure. 
 
---
## Goal of MDS 

 - Given pairwise dissimilarities, reconstruct a map that preserves distances.
 -  From any dissimilarity (no need to be a metric)
 -  Reconstructed map has coordinates $\mathbf{x}_i = (x_{i1}, x_{i2})$ and the
natural distance $\Vert x_i - x_j \Vert^2$.
 - MDS is a family of different algorithms, each designed to
arrive at optimal low-dimensional configuration (p = 2 or 3)
 - Includes: Classical MDS, Metric MDS and Non-metric MDS. 
 
---
## Examples first - Classical MDS

  - Problem: Given Euclidean Distance between points, recover the position of the points. 
 - Example: Road distance between 21 european cities

```{r}
library(datasets); class(eurodist)
```

```{}
                Athens Barcelona Brussels Calais Cherbourg Cologne
Barcelona         3313                                            
Brussels          2963      1318                                  
Calais            3175      1326      204                         
Cherbourg         3339      1294      583    460                  
Cologne           2762      1498      206    409       785  
```


---
## Output of MDS 

```{r, fig.asp = 0.8}
eurocmd <- cmdscale(eurodist)
plot(eurocmd, type = "p", pch = 16, col = "red")
text(eurocmd, rownames(eurocmd), adj = c(0,0.5), col = "red", cex = 0.8)
```


---
## Do we recover? 

```{r, echo = F, out.height = "400px", out.width = "600px"}
knitr::include_graphics("euromap.jpg")
```

- Can identify points up to shift, reflection and rotation. 


---
## Flip Axes 

```{r, fig.asp = 0.9}
plot(eurocmd[,1], -eurocmd[,2], type = "p", asp = 1,pch = 16, col = "red")
text(eurocmd[,1], -eurocmd[,2], rownames(eurocmd),, adj = c(0,0.5), col = "red", cex = 0.8)
```

- Can identify points up to shift, reflection and rotation.

---
## Another Example 

 - Air pollution in US Cities 
```{r}
data("USairpollution", package = "HSAUR2")
summary(USairpollution)
```
 - Some variables have larger range - need to standardise. 

---
## Try MDS at 2-D 

```{r, echo = T}
dat <- USairpollution # less typing
xs <- scale(dat)
pol.mds <- cmdscale(dist(xs), k = 2, eig = TRUE)
head(pol.mds$points)
```

-  You can think of these as coordinates in some **latent** space. 

---
## Distance $\ne$ Physical Distance

```{r, echo = F, fig.asp = 0.9, fig.align = 'center'}
x <- pol.mds$points
plot(x[,1], x[,2], type = "p", pch = 16, col = "red")
text(x[,1], x[,2], labels = rownames(x), adj = c(0.5, 0.5))
```

- SLC and Dallas have similar pollution profile. Chicago is an outlier. 

---
class: split-50 

## To scale or not to scale 
4 persons data on age and height. 

.pull-left[
```{r, echo = F}
dat <- data.frame(rbind(c("A",35,190),c("B",40,190),
                        c("C",35,160),c("D",40,160)))
colnames(dat) <- c("Person", "Age [years]", "Height [cm]")
library(knitr)
kable(dat)
```
]
.pull-right[
```{r, echo = F}
knitr::include_graphics("scale-1.jpg")
```
]

---
class: split-50

## To scale or not to scale 

.pull-left[
```{r, echo = F}
dat <- data.frame(rbind(c("A",35,6.232),c("B",40,6.232),
                        c("C",35,5.248),c("D",40,5.248)))
colnames(dat) <- c("Person", "Age [years]", "Height [ft]")
library(knitr)
kable(dat)
```
]

.pull-right[
Convert cm to feet: different subgroups emerge. 
```{r, echo = F}
knitr::include_graphics("scale-2.jpg")
```
]


---
class: split-50

## To scale or not to scale 

.pull-left[
```{r, echo = F}
dat <- data.frame(rbind(c("A",-0.87, 0.87),c("B",0.87, 0.87),
                        c("C",-0.87,-0.87),c("D",0.87,-0.87)))
colnames(dat) <- c("Person", "Age [years]", "Height [cm]")
library(knitr)
kable(dat, align = 'ccc', padding = 0, booktabs = T)
```
]

.pull-right[
```{r, echo = F}
knitr::include_graphics("scale-3.jpg")
```
]

Now all data-points are scaled : no subgroups.

---
## Context is important 

Which of the two representations make sense? 

```{r, echo = F, fig.align='center', out.width = 500}
dat <- data.frame(rbind(c("A",13.3,38.0),c("B",12.4,45.4),
                        c("C",-122.7,45.6),c("D",-122.4,37.7)))
colnames(dat) <- c("Object", "X1", "X2")
kable(dat, align = 'ccc', padding = 0, booktabs = T)

knitr::include_graphics("scale-4.jpg")
```


---
## Context is important 

You need knowledge of the context. Scaling is not a good idea always. 

```{r, echo = F, fig.align='center'}
dat <- data.frame(rbind(c("Palermo",13.3,38.0),c("Venice",12.4,45.4),
                        c("Portland",-122.7,45.6),c("San Francisco",-122.4,37.7)))
colnames(dat) <- c("Object", "Longitude", "Latitude")
kable(dat, align = 'ccc', padding = 0, booktabs = T)

knitr::include_graphics("scale-4.jpg")
```

---
## Thoery of MDS 

 - Given a dissimilarity matrix $D = (d_{ij})$, MDS seeks to find $x_1, \ldots, x_n \in \mathbb{R}^p$, such that: 
$$
d_{ij} \approx \Vert x_i - x_j \Vert^2 \text{ as close as possible}
$$

-  Often times, for some large $p$, there exists a configuration $x_1, \ldots, x_n$ with exact distance match $d_{ij} = \Vert x_i - x_j \Vert^2$. 
 -  In such a case $d$ is called a Euclidean distance.
 -  There are, however, cases where the dissimilarity is distance, but
there exists no configuration in any $p$ with perfect match. 
 - Such a distance is called non-Euclidean distance.

---
## Classical MDS: Thoery (continued)

 - Suppose for now we have Euclidean distance matrix $D = (d_{ij})$
 - We want to find $(x_1, \ldots, x_n)$ such that $d_{ij} = dist(x_i,x_j)$.
 - Not unique solution: $x^* = x + c$. 
 - Assume the observations are centered. 
 
---
## Thoery - Step 1

-  Find inner product matrix $B = X X^T$ instead of $X$. 
-  Connect to distance: $d_{ij}^2 = b_{ii}+b_{jj}-2b_{ij}$.
-  Center points to avoid shift invariance. 
-  Invert relationship: 

$$
\begin{equation}
b_{ij} = -1/2(d_{ij}^2 - d_{i.}^2 - d_{.j}^2 + d_{..}^2)
\end{equation}
$$
- "Doubly centered" distance. 
  
---
## Theory - Step 2

-  Since $B = X X^T$, we need the square-root of $B$. 
-  $B$ is symmetric and positive definite. 
-  Can be diagonalised: $B = V \Lambda V^T$. 
-  $\Lambda$ is diagonal matrix with eigenvalues $\lambda_1 > \lambda_2 > \cdots > \lambda_n$ 
-  Some eignevalues are zero, drop them. $B = V_1 \Lambda_1 V_1^T$.
-  Take "square-root" $X = V_1 \Lambda_1^{1/2}$. 
 
---
## Classical MDS 

- Want a 2-D plot. 
- Keep only largest 2 eignevectors and eignevalues. - The resulting $X$ will be the low-dimensional representation we were looking for. 
- Same for 3-D. 
 
---
## Classical MDS: Low-dim representation

-  Goodness of fit (GOF) if we reduce to m dimensions:

$$
\begin{equation}
\text{GOF}_{m} = \frac{\sum_{i = 1}^{m} \lambda_i}{\sum_{i = 1}^{n} \lambda_i}
\end{equation}
$$

-  We can choose $m$ dimensional representation if $\text{GOF}_{m} \ge 0.8$. 

---
## Classical MDS: Pros and Cons

Pros:

-   Optimal for euclidean input data
-   Still optimal, if $B$ has non-negative eigenvalues (pos. semidefinite)
-   Very fast

Cons: 
-   No guarantees if $B$ has negative eigenvalues. 


However, in practice, it is still used then. New measures for Goodness of fit:

$$
\begin{equation}
\text{GOF}_{m} = \frac{\sum_{i = 1}^{m} \vert \lambda_i \vert}{\sum_{i = 1}^{n} \vert \lambda_i \vert} \; \text{or} \; \text{GOF}_{m} = \frac{\sum_{i = 1}^{m} \Vert \lambda_i \Vert_{\infty}}{\sum_{i = 1}^{n} \Vert \lambda_i \Vert_{\infty}}
\end{equation}
$$

-  These are used in the `R` function `cmdscale`. 

---
## Non-metric MDS: Idea

-  Sometimes, there is no strict metric on original point
-   Example: how well do you like something? (1 - not at all, 10 - very much)

Subject | Teacher-1 | Teacher-2 | Teacher-3 
------- | --------- | --------- | ---------
S1      | 1         | 5         | 10
s2      | 2         | 6         | 9 
S3      | 1         | 7         | 8

-  Teacher 1 < Teacher 2 < Teacher 3 

---
## Non-metric MDS 

-  Absolute values are not that meaningful

-  Ranking is important

-  Non-metric MDS finds a low-dimensional representation, which respects the ranking of distances. 

---
## Non-metric MDS : Theory 

-  $\delta_{ij}$ is the true dissimilarity: $d_{ij}$ is the distance of representation. 

-  Minimize Stress: 
$$
\begin{equation}
S = \frac{\sum_{i <j} (\theta(\delta_{ij}) - d_{ij})^2}{\sum_{i <j} d_{ij}^2}, \; \theta \text{ is an increasing function}.
\end{equation}
$$ 

-  Optimize over "both" positions of points and $\theta$. 

-  $\hat{d_{ij}} = \theta(\delta_{ij})$ is called "disparity".

-  Solved numerically (isotonic regression); Classical MDS as starting value; 

-   very time consuming

---
## Example for Intuition 

```{r, echo = F}
knitr::include_graphics("non-metric-1.jpg")
```

```{r, echo = F}
knitr::include_graphics("non-metric-2.jpg")
```

```{r, echo = F}
knitr::include_graphics("non-metric-3.jpg")
```

---
##  Non-metric MDS: Pros and Cons


+ Fulfills a clear objective without many assumptions (minimize STRESS)

+ Results donâ€™t change with rescaling or monotonic variable transformation
+ Works even if you only have rank information


- Slow in large problems
- Usually only local (not global) optimum found
- Only gets ranks of distances right

---
# Voting Data 

 - Look at the voting data. 
 - Are Democrats close to each other, republicans?

```{r}
data("voting", package = "HSAUR")
require("MASS")
head(voting[,1:4], n = 4)
```

---
## Metric MDS

-   For metric MDS, use function `cmdscale`. 
```{r}
voting.mds <- cmdscale(voting, eig=TRUE)
names(voting.mds)
```


---
## Plot 

-  What do the coordinates represent? 

```{r, echo = F, eval = T}
demind <- grep("(D)",colnames(voting))
col <- rep("red",15)
col[demind] <- "blue"

plot(voting.mds$points[,1], voting.mds$points[,2],
     type = "n", xlab = "Coordinate 1", ylab = "Coordinate 2",
     xlim = range(voting.mds$points[,1])*1.2)
text(voting.mds$points[,1], voting.mds$points[,2], 
     labels = colnames(voting), cex = 0.7, col = col)
```

---
## Solution - non-metric

```{r}
voting_mds <- isoMDS(voting)
```


---
## Plot the non-metric MDS 

```{r, echo = F, eval = T}
demind <- grep("(D)",colnames(voting))
col <- rep("red",15)
col[demind] <- "blue"

plot(voting_mds$points[,1], voting_mds$points[,2],
     type = "n", xlab = "Coordinate 1", ylab = "Coordinate 2",
     xlim = range(voting_mds$points[,1])*1.2)
text(voting_mds$points[,1], voting_mds$points[,2], 
     labels = colnames(voting), cex = 0.7, col = col)
```

---
## Use `ggrepel` 

-  Exercise: how would you add colour to the text labels?

```{r, echo = T, eval = F}
library(ggplot2)
library(ggrepel)

demind <- grep("(D)",colnames(voting))
col <- rep("red",15)
col[demind] <- "blue"

voting.data <- rbind(data.frame(voting_mds$points))

set.seed(42)
ggplot(voting.data) +
  geom_point(aes(X1,X2)) +
  geom_text_repel(aes(X1,X2, 
                      label = rownames(voting.data))) +
  theme_classic(base_size = 16)
```

---
## Use `ggrepel` fo better plots 

```{r, echo = F, eval = T, warning = F, message = F}
library(ggplot2)
library(ggrepel)

voting.data <- data.frame(voting_mds$points)

set.seed(42)
ggplot(voting.data) +
  geom_point(aes(X1,X2), color = 'red') +
  geom_text_repel(aes(X1,X2, 
                      label = rownames(voting.data))) +
  theme_classic(base_size = 16)
```

---
## Main message 

### Classical MDS:


-  Finds low-dim projection that respects distances -  Optimal for euclidean distances 
-  No clear guarantees for other distances 
-  fast


### Non-metric MDS:

-  Squeezes data points on table 
-  respects only rankings of distances 
-  (locally) solves clear objective 
-  slow

---
class: inverse, center, middle

## Random forest for MDS 

---
# Random Forest and MDS 

-   In absence of a distance measure, we can use random forest to get a proximity measure. 
-   Proximity measure:  The $(i,j)$ element of the proximity matrix produced by `randomForest` is the fraction of trees in which elements $i$ and $j$ fall in the same terminal node. 
-   The intuition is that **similar** observations should be in the
same terminal nodes more often than dissimilar ones. 
-   We can use the proximity measure to identify structure. 

---
# Random forest on `iris` data


```{r, warning = F, message = F}
library(randomForest)
ind <- sample(2,nrow(iris),replace=TRUE,prob=c(0.7,0.3))
trainData <- iris[ind==1,]
testData <- iris[ind==2,]
iris_rf <- randomForest(Species~.,data=trainData,ntree=100,proximity=TRUE)
table(predict(iris_rf),trainData$Species)
```

---
# Random forest on `iris` data

.pull-left[
```{r}
varImpPlot(iris_rf)
```
]

.pull-right[

```{r}
plot(iris_rf)
```
]

---
## Prediction on new data 

```{r}
irisPred<-predict(iris_rf,newdata=testData)
table(irisPred, testData$Species)
```

---
## MDS on the full Iris Data


```{r}
set.seed(71)
iris.rf <- randomForest(Species ~ ., data=iris, importance=TRUE,
                        proximity=TRUE)
## Do MDS on 1 - proximity:
iris.mds <- cmdscale(1 - iris.rf$proximity, eig=TRUE)
## Look at variable importance:
round(importance(iris.rf), 2)
```

---
## Plot the MDS coordinates

```{r, echo = F}
op <- par(pty="s")
plot(iris.mds$points[,1],iris.mds$points[,2],
      col=c("red", "green", "blue")[as.numeric(iris$Species)],
      main="Iris Data: MDS of Proximity Based on RandomForest")
par(op)
```

---
## Plot the MDS coordinates with the original variables 

```{r, echo = F}
op <- par(pty="s")
pairs(cbind(iris[,1:4], unlist(iris.mds$points)), cex=0.6, gap=0,
      col=c("red", "green", "blue")[as.numeric(iris$Species)],
      main="Iris Data: Predictors and MDS of Proximity Based on RandomForest")
par(op)
```
