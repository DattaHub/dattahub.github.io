<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Matrix Operations in R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jyotishka Datta" />
    <meta name="date" content="2025-02-04" />
    <script src="matrix_algebra_x_files/header-attrs-2.28/header-attrs.js"></script>
    <link href="matrix_algebra_x_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="matrix_algebra_x_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Matrix Operations in R
]
.author[
### Jyotishka Datta
]
.date[
### 2025-02-04
]

---


  


&lt;style type="text/css"&gt;
pre {
  background: #FFBB33;
  max-width: 100%;
  overflow-x: scroll;
}
&lt;/style&gt;

---
## About this Chapter
- Not a comprehensive survey of linear algebra.


---
## Scalars
- A scalar is a single number.
- Examples: integers, real numbers, rational numbers.
- Denoted using italic font: `\(a, n, x\)`.
- In `R`, (almost) everything is a vector.


``` r
a &lt;- 3 
a
```

```
## [1] 3
```

- `a` is a scalar but R treats it as a length-1 vector


``` r
a[1]
```

```
## [1] 3
```

``` r
length(a)
```

```
## [1] 1
```

``` r
is.vector(a)
```

```
## [1] TRUE
```

---
## Vectors
- A vector is a 1-D array of numbers.
- Notation: `\(\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\)`.
- Represent points in space. 
- Usual notation `\(\mathbf{a} \in \mathbb{R}^n\)`.

---
## R Example: Vectors

-  We can use `c` function to create vectors in `R`, or use pre-defined functions like `seq`. 


``` r
v &lt;- seq(2,8,2)
v
```

```
## [1] 2 4 6 8
```

``` r
length(v)
```

```
## [1] 4
```

``` r
is.vector(v)
```

```
## [1] TRUE
```

``` r
w &lt;- 4:1
w
```

```
## [1] 4 3 2 1
```

---
## Vector arithmetic 

- We can add two vectors `\(\bf{x}\)` and `\(\bf{y}\)` using the same notation `x + y`.
 

``` r
v + w
```

```
## [1] 6 7 8 9
```
-  The inner product of a vector is defined as `\(\bf{x}^T \bf{x} = \sum_i x_i^2\)`, and can be calculated as: 


``` r
crossprod(v)
```

```
##      [,1]
## [1,]  120
```
-  Notice the `crossprod` function returns a 1 × 1 matrix, not a scalar.

---
## Componentwise product vs `crossprod`

-  Similarly, the cross-product `\(\bf{x}^T \bf{y}\)` can be done by:


``` r
crossprod(v,w)
```

```
##      [,1]
## [1,]   40
```

-  The `*` operation for vectors performs a component-wise product of vectors:


``` r
v*w
```

```
## [1]  8 12 12  8
```


---
## Outer product 

The outer product `\({\bf x} {\bf x}^T\)` is an `\(n \times n\)` matrix. This is computed using the `%o%` operator:


``` r
v %o% v
```

```
##      [,1] [,2] [,3] [,4]
## [1,]    4    8   12   16
## [2,]    8   16   24   32
## [3,]   12   24   36   48
## [4,]   16   32   48   64
```

- Notice how `R` prints the matrix values, bordered by a useful index notation.


``` r
(v %o%v)[,3]
```

```
## [1] 12 24 36 48
```


## Matrices

- A matrix is a 2-D array of numbers.
- Notation: `\(A \in \mathbb{R}^{m \times n}\)`.
- Example:
  $$ A = \begin{bmatrix} a_{1,1} &amp; a_{1,2} \\ a_{2,1} &amp; a_{2,2} \end{bmatrix} $$
  
-  Obviously, `\(a_{ij}\)` or `\(a_{i,j}\)` denotes the element on the `\(i^{th}\)` row and `\(j^{th}\)` column.

## Data matrix   

- We usually express "data" in matrix language, for example, if we have `\(p\)` predictors on `\(n\)` subjects, we can denote the data as a `\(n \times p\)` matrix `\(\bf{X}\)`.

$$
X = \begin{bmatrix} x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1p} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2p} \\
\ldots &amp; \ldots &amp; \vdots &amp; \ldots \\
x_{n1} &amp; x_{n2} &amp; \ldots &amp;x_{np} 
\end{bmatrix}
$$
- The mean vector or centroid could be written as:

$$ \bar{\bf x} = \begin{pmatrix} \bar{x}_1 \\ \bar{x}_2 \\ \vdots \\ \bar{x}_n \end{pmatrix} \in \mathbb{R}^n$$

## R Example: Matrices



``` r
data(mtcars)
head(mtcars)
```

```
##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
```

``` r
dim(mtcars)
```

```
## [1] 32 11
```

## Matrices  

- To arrange values into a matrix, we use the `matrix()` function:


``` r
A &lt;- matrix(1 : 6, nrow = 2, ncol = 3)
A
```

```
##      [,1] [,2] [,3]
## [1,]    1    3    5
## [2,]    2    4    6
```

- Individual entries can be referred to using a pair of indices. For example, the element in the second row, third column can be printed as:


``` r
A[2, 3]
```

```
## [1] 6
```

## Rows, Columns 
-  An entire row or column can be retrieved by specifying its index and leaving the other index empty:


``` r
A[2, ]
```

```
## [1] 2 4 6
```
-  Similarly to the way R works with vectors, you can use negative indices to exclude row or colums of a matrix:


``` r
A[-2,]
```

```
## [1] 1 3 5
```

## Matrix indices 

- Note how the values 1,2,3,4,5,6 are used to create the matrix: the first two are used to fill the first column, then the next two to fill the second column, and so on.

- R allows matrices to be indexed by a single number, e.g.,


``` r
A[5]
```

```
## [1] 5
```
- The fifth element of a is the fifth element of the vector obtained by "unrolling" the matrix, column by column. 

## Fill by row 

-  Sometimes you need to fill a matrix row by row, instead than column by column. 
-  You can change the default behavior as follows:


``` r
A &lt;- matrix(1 : 6, nrow = 2, ncol = 3, byrow = TRUE)
A
```

```
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
```

## Matrix Transpose {.smaller}

- The transpose of a matrix `\(A\)` is denoted as `\(A^T\)`.
- Example:
  $$ A = \begin{bmatrix} a_{1,1} &amp; a_{1,2} &amp; a_{1,3} \\ a_{2,1} &amp; a_{2,2} &amp; a_{2,3} \end{bmatrix} \Rightarrow  A^T = \begin{bmatrix} a_{1,1} &amp; a_{2,1} \\ a_{1,2} &amp; a_{2,2} \\ a_{1,3} &amp; a_{2,3} \end{bmatrix} $$

-  You can think of transpose as mirror image across the main diagonal. Rows of `\(A\)` become columns of `\(A^T\)` and vice versa. 

-  Useful: `\((A B)^T = B^T A^T\)`. 

-  `\(A\)` is called a symmetric matrix if `\(A^T = A\)`. 

## R Example: Matrix Transpose


``` r
matrix_A &lt;- matrix(c(1, 2, 3, 4), nrow=2, ncol=2)
print(matrix_A)
```

```
##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4
```

``` r
t_A &lt;- t(matrix_A)
print(t_A)
```

```
##      [,1] [,2]
## [1,]    1    2
## [2,]    3    4
```

-  The `t()` function takes transpose of a vector, into an `\(1 \times n\)` matrix, so you could also do `crossprod` this way:


``` r
t(v)%*%w
```

```
##      [,1]
## [1,]   40
```


## Matrix Multiplication

- If `\(A \in \mathbb{R}^{m \times n}\)` and `\(B \in \mathbb{R}^{n \times p}\)`, then `\(C = AB\)` is defined.
- Each element: `\(c_{i,j} = \sum_{k} a_{i,k} b_{k,j}\)`.


![](matrixmultiplication.png){width="10%"}


## R Example: Matrix Multiplication

- Matrix multiplication in R is performed by the operator %*%. For example,


``` r
A &lt;- matrix(c(1, 2, 3, 4), nrow=2)
B &lt;- matrix(c(2, 0, 1, 2), nrow=2)
C &lt;- A %*% B
print(C)
```

```
##      [,1] [,2]
## [1,]    2    7
## [2,]    4   10
```

## Multiplication  

- Matrices must conform, i.e. multiplication must make sense !


``` r
A &lt;- matrix(c(1, 2, 0, 1), 2)
B &lt;- matrix(c(1, 3, 3, 1, 4, 5), nr = 2)
A
```

```
##      [,1] [,2]
## [1,]    1    0
## [2,]    2    1
```

``` r
B
```

```
##      [,1] [,2] [,3]
## [1,]    1    3    4
## [2,]    3    1    5
```


``` r
A %*% B
```

```
##      [,1] [,2] [,3]
## [1,]    1    3    4
## [2,]    5    7   13
```


## Multiplication  

-  `\(A\)` is a `\(2 \times 2\)` matrix and `\(B\)` is a `\(2 \times 3\)` matrix, so `\(AB\)` can be computed but not `\(BA\)`!


``` r
B %*% A
```

```
## Error in B %*% A: non-conformable arguments
```

``` r
dim(A)
```

```
## [1] 2 2
```

``` r
dim(B)
```

```
## [1] 2 3
```

## Multiplication 

-  With `\(A\)` and `\(B\)` defined as above the matrix product `\(BA\)` in not defined, but the product `\(B^T A\)` is ($B^T$ denotes the transpose of `\(B\)`). 
- The transpose of a matrix can be obtained with the function `t()`.

``` r
t(B) %*% A
```

```
##      [,1] [,2]
## [1,]    7    3
## [2,]    5    1
## [3,]   14    5
```

## Crossprod

-  A much more efficient way of computing the same matrix product is via the function `crossprod()`.

``` r
all.equal(crossprod(B, A), t(B) %*% A)
```

```
## [1] TRUE
```
- A similar function, tcrossprod(), computes the matrix product `\(AB^T\)` whenever the product is well defined.

## Exercise (Try at home)  

- Consider the matrix

``` r
X &lt;- cbind(1, seq(-1, 1, length = 11))
X
```

```
##       [,1] [,2]
##  [1,]    1 -1.0
##  [2,]    1 -0.8
##  [3,]    1 -0.6
##  [4,]    1 -0.4
##  [5,]    1 -0.2
##  [6,]    1  0.0
##  [7,]    1  0.2
##  [8,]    1  0.4
##  [9,]    1  0.6
## [10,]    1  0.8
## [11,]    1  1.0
```
- Use crossprod() and tcrossprod() to find `\(X^T X\)` and `\(X X^T\)`. 
- Note the dimensions of the resulting matrices.


## Identity Matrix

- The identity matrix `\(I_n\)` satisfies `\(I_n x = x\)` for any `\(x\)`.
- Example:
  $$ I_3 = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} $$

``` r
I3 &lt;- diag(3)
print(I3)
```

```
##      [,1] [,2] [,3]
## [1,]    1    0    0
## [2,]    0    1    0
## [3,]    0    0    1
```




## Special matrices 

-  *Triangular* Upper triangular when `\(A_{ij} = 0\)` for `\(i&lt;j\)`, elements above diagonal are zero. Similarly, lower triangular when `\(A_{ij} = 0\)` for `\(i&gt;j\)`. 
-   *Diagonal and tridiagonal* Diagonal matrices have all non-diagonal entries zero ($A_{ij}=0$ for `\(i \ne j\)`) and tridiagonal matrices have `\(A_{ij}=0\)` for `\(|i-j|&gt;1\)`.
-   *Positive definite* `\(A\)` symmetric with `\(x^{T}A x &gt; 0\)` for all `\(x \ne 0\)`
-   *Orthogonal* `\(A^{T}A = I\)`.

## The `diag` function 

:::: {.columns}

::: {.column width="50%"}

-  The `diag` function can be used to both build diagonal matrices as well as extract the diagonal elements of an existing matrix.


``` r
t(matrix(1 : 16, 4, 4))
```

```
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    5    6    7    8
## [3,]    9   10   11   12
## [4,]   13   14   15   16
```

:::

::: {.column width="50%"}


``` r
diag(t(matrix(1 : 16, 4, 4)))
```

```
## [1]  1  6 11 16
```

- Put another `diag` if you want the diagnoal elements as a diagonal matrix. 


``` r
diag(diag(t(matrix(1 : 16, 4, 4))))
```

```
##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    0
## [2,]    0    6    0    0
## [3,]    0    0   11    0
## [4,]    0    0    0   16
```

:::

::::

## Diagonals {.smaller}

- We have seen `rbind()` and `cbind()` before. 


``` r
rbind(1:4, 2:5)
```

```
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    2    3    4    5
```

- e.g. We can use `cbind()` to construct a 3 x 3 Hilbert matrix. The (i, j) entry is
`\(1/(i + j - 1).\)`


``` r
H3 &lt;- 1 / cbind(1 : 3, 2 : 4, 3 : 5)
H3
```

```
##           [,1]      [,2]      [,3]
## [1,] 1.0000000 0.5000000 0.3333333
## [2,] 0.5000000 0.3333333 0.2500000
## [3,] 0.3333333 0.2500000 0.2000000
```

- The Hilbert matrices are canonical examples of ill-conditioned matrices, being notoriously difficult to use in numerical computation. 

## Diagonals and Determinants

- The determinant of a square matrix can be obtained with `det()`.


``` r
det(H3)
```

```
## [1] 0.000462963
```
- For a diagonal matrix determinant is the product of the diagonal elements. 


``` r
A = diag(1:4)
A
```

```
##      [,1] [,2] [,3] [,4]
## [1,]    1    0    0    0
## [2,]    0    2    0    0
## [3,]    0    0    3    0
## [4,]    0    0    0    4
```

``` r
det(A)
```

```
## [1] 24
```


## Triangular 

-  The functions `lower.tri()` and `upper.tri()` can be used to obtain the lower and upper triangular parts of matrices. 

-  The output of the functions is a matrix of logical elements, with TRUE representing the relevant triangular elements. For example,


``` r
lower.tri(H3)
```

```
##       [,1]  [,2]  [,3]
## [1,] FALSE FALSE FALSE
## [2,]  TRUE FALSE FALSE
## [3,]  TRUE  TRUE FALSE
```

## Triangular 

- A typical use of these functions is to set the upper or lower triangular part of a matrix to zero, thus constructing a triangular matrix.


``` r
Htri &lt;- H3
Htri[lower.tri(Htri)] &lt;- 0
Htri
```

```
##      [,1]      [,2]      [,3]
## [1,]    1 0.5000000 0.3333333
## [2,]    0 0.3333333 0.2500000
## [3,]    0 0.0000000 0.2000000
```


## Eigendecomposition

Aneigenvectorof a square matrix A is a nonzero vector v such that multiplication by A alters only the scale of v:

-  Eigenvector `\(v\)`: `\(Av = \lambda v\)`.

-  The scalar `\(\lambda\)` is known as the eigenvalue corresponding to this eigenvector.


## Eigen-decomposition

-  Suppose that a matrix `\(\mathbf{A}\)` has `\(n\)` linearly independent eigenvectors `\(\{\mathbf{v}^{(1)}, \ldots, \mathbf{v}^{(n)}\}\)` with corresponding eigenvalues 
`\(\{\lambda_1, \ldots, \lambda_n\}\)`. 

-  We may concatenate all the eigenvectors to 
form a matrix `\(\mathbf{V}\)` with one eigenvector per column: 
`\(\mathbf{V} = [\mathbf{v}^{(1)}, \ldots, \mathbf{v}^{(n)}]\)`. 

-  Likewise, we can 
concatenate the eigenvalues to form a vector $\boldsymbol{\lambda} = [\lambda_1, 
\ldots, \lambda_n]^\top$. 

-  The eigendecomposition of `\(\mathbf{A}\)` is then given by

$$
\mathbf{A} = \mathbf{V} \, \mathrm{diag}(\boldsymbol{\lambda}) \, \mathbf{V}^{-1}.
$$
## Effect of eigenvectors 

![](eigenvector.png){width="60%"}

## R Example: Eigendecomposition


``` r
A &lt;- matrix(c(3, 1, 1, 3), nrow=2)
eigen_A &lt;- eigen(A)
print(eigen_A)
```

```
## eigen() decomposition
## $values
## [1] 4 2
## 
## $vectors
##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068
```

## Singular Value Decomposition (SVD)

- Similar to eigendecomposition but more general.
- Every real matrixhas an SVD: `\(A = UDV^T\)`.
- Used in dimensionality reduction, pseudoinverse computation.

## R Example: SVD


``` r
svd_A &lt;- svd(A)
print(svd_A)
```

```
## $d
## [1] 4 2
## 
## $u
##            [,1]       [,2]
## [1,] -0.7071068 -0.7071068
## [2,] -0.7071068  0.7071068
## 
## $v
##            [,1]       [,2]
## [1,] -0.7071068 -0.7071068
## [2,] -0.7071068  0.7071068
```


## Trace Operator
- The trace of a square matrix is the sum of its diagonal elements.
- Notation: `\(\text{Tr}(A) = \sum_{i} A_{i,i}\)`.
- Useful in matrix calculus and optimization.

## R Example: Trace

``` r
tr_A &lt;- sum(diag(A))
print(tr_A)
```

```
## [1] 6
```

## Matrix inversion

- The inverse of a matrix can be found using `solve()`:


``` r
Ainv &lt;- solve(A)
Ainv
```

```
##        [,1]   [,2]
## [1,]  0.375 -0.125
## [2,] -0.125  0.375
```

``` r
all.equal(Ainv %*% A, diag(2))
```

```
## [1] TRUE
```

## Linear systems

-  Many common problems in Statistics require solving a linear system of equations:

$$
A x = b. \label{eq:ls}
$$


-  Examples: Least squares normal equations for linear regression: `\((X^{T}X)\beta = X^{T}y\)` 

-  or the stationary distribution of a Markov chain `\(\pi P = \pi\)`, where `\(\sum \pi = 1\)`. 


## Linear systems

- A typical linear model can be written in matrix form as: 

$$
\begin{bmatrix}
        y_1 \\ y_2 \\ ... \\ y_n
    \end{bmatrix} = 
    \begin{bmatrix}
        x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\ x_{21} &amp; x_{22} &amp; ... &amp; x_{2p} \\ ... &amp; ... &amp; ... &amp; ...\\ x_{n1} &amp; x_{n2} &amp; ... &amp; x_{np}
    \end{bmatrix} \times \begin{bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix}
$$

## Ordinary least squares 


- The ordinary least squares (OLS) method is used to estimate the `\(p\times 1\)` parameter vector `\(\boldsymbol{\beta}\)` of the linear regression model `\(\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}\)`.

-  The estimator `\(\boldsymbol{\hat{\beta}_{\text{OLS}}}\)` is defined as:

$$
\begin{equation*}
    \boldsymbol{\hat \beta}_{\text{OLS}} = \underset{\boldsymbol{\beta}\in\mathbb{R}^p}{\operatorname{argmin}} \lVert \mathbf{Y}-\mathbf{X}\boldsymbol{\beta} \rVert ^2 = \underset{\boldsymbol{\beta}\in\mathbb{R}^p}{\operatorname{argmin}}\hspace{1mm}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})
\end{equation*}
$$
## Ordinary least squares 


-  That is, `\(\boldsymbol{\hat \beta}_{\text{OLS}}\)` is the choice of `\(\boldsymbol{\beta}\)` that minimizes the squared norm of the deviation (error) between the observed values contained in `\(\mathbf{Y}\)` and the estimated values contained in `\(\mathbf{X}\boldsymbol{\beta}\)`. 

## Ordinary least squares 

-  To derive `\(\boldsymbol{\hat \beta}_{\text{OLS}}\)`, let `\(f=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})\)`. Expanding `\(f\)` yields:

$$
\begin{align*}
    f &amp;= \mathbf{Y'Y}-\boldsymbol{\beta}'\mathbf{X'Y}-\mathbf{Y'X}\boldsymbol{\beta}+\boldsymbol{\beta}'\mathbf{X'X}\boldsymbol{\beta} \\
    &amp; = \mathbf{Y'Y}-2\boldsymbol{\beta}'\mathbf{X'Y}+\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}
\end{align*}
$$

## Ordinary least squares 


-  Differentiate to find the value of `\(\boldsymbol{\beta}\)` which minimizes `\(f\)`:

$$
\begin{align*}
    &amp;\frac{\boldsymbol{\delta} f}{\boldsymbol{\delta}\boldsymbol{\beta}} = -2\mathbf{X'Y}+2\mathbf{X'X}\boldsymbol{\beta} = \mathbf{0} \hspace{2mm}\rightarrow\hspace{2mm} \mathbf{X'X}\boldsymbol{\beta} = \mathbf{X'Y}\\
    &amp; \rightarrow \hspace{2mm} \boldsymbol{\hat \beta}_{\text{OLS}} = (\mathbf{X'X})^{-1}\mathbf{X'Y}
\end{align*}
$$

## Ordinary least squares 


Verify that the estimator `\(\boldsymbol{\hat \beta}_{\text{OLS}}\)` is a minimum by examining the matrix of second derivatives:

$$
\begin{align}\label{Hessian}
    \frac{\boldsymbol{\delta}^2f}{\boldsymbol{\delta\beta}^2} = 2\mathbf{X'X}
\end{align}
$$


-  For most regression problems in practice with `\(n &gt; p\)`, the matrix `\(X^TX\)` is positive definite, and thus it has positive eigenvalues and is nonsingular. `\(f\)` therefore attains a minimum at `\(\boldsymbol{\hat \beta}_{\text{OLS}}\)`.

-  Note that `\(\mathbf{X'X}\)` is a `\(p\times p\)` matrix, `\(\boldsymbol{\beta}\)` is a `\(p \times 1\)` vector, and  `\(\mathbf{X'Y}\)` is a `\(p \times 1\)` vector. 


## System of Equations

-  A linear system of equations can have:
   -  No solution
   -  Many solutions
   -  Exactly one solution: this means multiplication by the matrix is an invertible function. 

##  Linear system

-  If the matrix `\(A\)` is a square non-singular matrix, the usual solution to the `\(A x = b\)` is `\(x = A^{-1} b\)`.

$$
\begin{align}
A x &amp; = b \\
\Rightarrow A^{-1} A x &amp; = A^{-1} b \\
\Rightarrow I_n x &amp; = A^{-1} b \equiv x =  A^{-1} b.
\end{align}
$$


-  Directly inverting the matrix `\(A\)` is inefficient and could be numerically inaccurate `\(\Rightarrow\)` Gaussian elimination!

## Invertibility 

-  Matrix can’t be inverted if…
  -   More rows than columns
  -   More columns than rows
  -   Redundant rows/columns (“linearly dependent”,
“low rank”)

##  Invertibility 

-  For a square matrix, 
      -   invertibility is equivalent to 
      -   having a non-zero determinant, 
      -   full rank (meaning the rank equals the number of rows/columns), 
      -   and linearly independent columns (or rows); essentially, all these conditions imply the matrix can be inverted and vice versa. 
      
-  A square matrix with linearly dependent columns is known as singular. 

## Check for singular 


``` r
A &lt;- matrix(c(2, 4, 1, 2), nrow = 2, byrow = TRUE)

A
```

```
##      [,1] [,2]
## [1,]    2    4
## [2,]    1    2
```

``` r
det(A)
```

```
## [1] 0
```

``` r
qr(A)$rank  # QR decomposition gives rank
```

```
## [1] 1
```

``` r
(qr(A)$rank == ncol(A))
```

```
## [1] FALSE
```

      
## Check non-singular


``` r
A &lt;- matrix(c(2, 4, 1, 3), nrow = 2, byrow = TRUE)

A
```

```
##      [,1] [,2]
## [1,]    2    4
## [2,]    1    3
```

``` r
det(A)
```

```
## [1] 2
```

``` r
qr(A)$rank  # QR decomposition gives rank
```

```
## [1] 2
```

``` r
(qr(A)$rank == ncol(A))
```

```
## [1] TRUE
```


## Solve linear system 

- Inverse in R uses QR decomposition, that we might briefly talk about at the very end of this course (if time permits)! 

- For linear systems we don't need to compute inverses. We can use Gaussian Elimination. 


``` r
b &lt;- c(5, 3)
solve(A, b) # use this
```

```
## [1] 1.5 0.5
```

``` r
Ainv %*% b # don't use this
```

```
##      [,1]
## [1,]  1.5
## [2,]  0.5
```

## Time comparison 

- Compare execution times for the different methods of solving the Least Squares problem when the number of observations is large.

-  Let us start by making up our own random data: `\(n \times p\)` design matrix `\(X\)` and an `\(n \times 1\)` response vector `\(y\)`. 
-  Take `\(n = 5 \times 10^5\)`. 


``` r
set.seed(123)
n &lt;- 5e5; p &lt;- 20
X &lt;- matrix(rnorm(n * p, mean = 1 : p, sd = 10), nr = n, nc = p, byrow = TRUE)
y &lt;- rowSums(X) + rnorm(n)
```

## OLS 

- The naive approach to solve the LS problem is to compute: 
`\(\hat{\beta} = (X^T X)^{-1}X^T y\)`


``` r
system.time(bHat1 &lt;- solve(t(X) %*% X) %*% t(X) %*% y)
```

```
##    user  system elapsed 
##    0.08    0.00    0.30
```

-  Not so naive: use `crossprod`.


``` r
system.time(bHat2 &lt;- solve(crossprod(X), crossprod(X, y)))
```

```
##    user  system elapsed 
##    0.09    0.00    0.08
```


## Moore-Penrose Pseudoinverse

- Used when matrix inversion is not possible.
- Defined as: `\(A^+ = V D^+ U^T\)`.
- Provides least-squares solutions to systems of equations.

## R Example: Pseudoinverse


``` r
library(MASS)
pinv_A &lt;- ginv(A)
print(pinv_A)
```

```
##      [,1] [,2]
## [1,]  1.5   -2
## [2,] -0.5    1
```

## Variance-Covariance Matrix

- Measures the variance and covariance between variables.
- Notation: `\(S = \frac{1}{n-1} (X - \bar{X})(X - \bar{X})^T\)`

## R Example: Variance-Covariance Matrix


``` r
data_matrix &lt;- matrix(rnorm(20), nrow=5)
cov_matrix &lt;- cov(data_matrix)
print(cov_matrix)
```

```
##             [,1]        [,2]        [,3]        [,4]
## [1,]  1.16790978 -0.34095029 -0.43533046  0.03809341
## [2,] -0.34095029  0.39511208 -0.23842197 -0.06233475
## [3,] -0.43533046 -0.23842197  0.62003382  0.08145113
## [4,]  0.03809341 -0.06233475  0.08145113  0.36430614
```

## Correlation Matrix
- Standardized measure of association between variables.
- Notation: `\(R = D^{-1} S D^{-1}\)`

## R Example: Correlation Matrix


``` r
corr_matrix &lt;- cor(data_matrix)
print(corr_matrix)
```

```
##             [,1]       [,2]       [,3]        [,4]
## [1,]  1.00000000 -0.5019105 -0.5115719  0.05839988
## [2,] -0.50191048  1.0000000 -0.4817020 -0.16429973
## [3,] -0.51157194 -0.4817020  1.0000000  0.17137843
## [4,]  0.05839988 -0.1642997  0.1713784  1.00000000
```

## Standardization (Scaling and Centering)

- Subtract mean and divide by standard deviation.

## R Example: Standardization

``` r
scaled_data &lt;- scale(data_matrix)
print(scaled_data)
```

```
##            [,1]       [,2]       [,3]       [,4]
## [1,]  0.9706443 -0.6095492 -0.2509688  1.5088997
## [2,] -1.6927480  0.7694060  0.9848463  0.4785801
## [3,]  0.1392433  0.9140561 -1.0237930 -0.9713091
## [4,]  0.3314427  0.3642430 -0.8237823 -0.3768608
## [5,]  0.2514177 -1.4381559  1.1136978 -0.6393099
## attr(,"scaled:center")
## [1] -0.03878954  0.09938833 -0.31053876  0.06362857
## attr(,"scaled:scale")
## [1] 1.0806987 0.6285794 0.7874223 0.6035778
```


# More advanced topics  

## Inverting a matrix using Cholesky helps

- A symmetric positive definite matrix can be factorized as: 
$$
A = L L^T
$$


``` r
p = 1000; A = array(rnorm(p*p), c(p,p))
A = crossprod(A)
ptm &lt;- proc.time(); B = solve(A); proc.time()-ptm
```

```
##    user  system elapsed 
##    0.26    0.01    0.47
```

``` r
ptm &lt;- proc.time(); B = chol2inv(A); proc.time()-ptm
```

```
##    user  system elapsed 
##    0.10    0.00    0.11
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
