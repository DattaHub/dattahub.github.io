---
title: "PCA"
author: "Jyotishka Datta (<jyotishka@vt.edu>) <br> Department of Statistics, Virginia Tech"
output: 
  ioslides_presentation:
    smaller: yes
    logo: ../vt.png
    transition: faster
css: custom.css
---

```{r setup, include=FALSE}
options(width=80)
library(knitr)
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
knit_hooks$set(no.main = function(before, options, envir) {
    if (before) par(mar = c(4.1, 4.1, 1.1, 1.1))  # smaller margin on top
})
```


## Source 

-  A lot of R codes in this demo are taken from R codes in the excellent book "Introduction to Statistical Learning with Applications in R" by James, Witten, Hastie and Tibshirani. The Book website has R codes, labs and lots of other resources [http://www.statlearning.com/](http://www.statlearning.com/).

- The Crime data and Iris data codes were provided by Dr. Sumanta Basu. PCR codes & examples were provided by Dr. Bobby Gramacy. 

## Recap (PCA) {.build}

-  In practice, we observe $n$ observations of the $(x_1, \ldots, x_p)$.
-  Let $X$ be a data matrix with $n$ rows and $p$ columns. Subtract the column mean from each variable. 
-  The first principal component direction is the vector $\beta_1 = (\beta_{11}, \ldots, \beta_{p1})'$ s.t. 
$$
		\max_{\beta_{11}, \ldots, \beta_{p1}} \left\{ \frac{1}{n} \left( \sum_{j=1}^{p} \beta_{j1} x_{ij} \right)^2 \right\}
		\text{ subject to } \sum_{j=1}^{p}\beta_{j1}^2  = 1 
$$

## Recap (PCA)

$$
		\max_{\beta_{11}, \ldots, \beta_{p1}} \left\{ \frac{1}{n} \left( \sum_{j=1}^{p} \beta_{j1} x_{ij} \right)^2 \right\}
		\text{ subject to } \sum_{j=1}^{p}\beta_{j1}^2  = 1 
$$

-  $\sum_{j=1}^{p} \beta_{j1} x_{ij}$: Projection of $i^{th}$ sample into $\beta_1$ - also called score $y_{i1}$.
- Maximize: Sample variance of the scores $y_{i1}$.

## Second PC

-  The second principal component direction is the vector $\beta_2 = (\beta_{12}, \ldots, \beta_{p2})'$ s.t. 
$$
		\max_{\beta_{12}, \ldots, \beta_{p2}} \left\{ \frac{1}{n} \left( \sum_{j=1}^{p} \beta_{j2} x_{ij} \right)^2 \right\} \\
		\text{ subject to } \sum_{j=1}^{p}\beta_{j2}^2  = 1, \sum_{j=1}^{p}\beta_{j1} \beta_{j2} = 0 
$$
-  First and second principal components must be orthogonal.
-  Scores $(y_{i1}, i = 1, \ldots, n)$ and $(y_{i2}, i = 1, \ldots, n)$ are independent. 


## Optimization for PCA

- PCA can be performed in two different ways: 
- The singular value decomposition of $X$: 
$$ 
X = U \Sigma B^{T}
$$ 
where the $i$-th column of $B$ is the $i$-th principal component direction $\beta_i$. 
- The Eigen-Decomposition of $X^TX$:
$$
X^T X = B \Sigma^2 B^T 
$$ 

## Using R 

- Two packages `prcomp` and `princomp`. 
- Difference: 

prcomp: The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy.

primcomp: uses Eigen-decomposition. 

## US Arrest data 

Reminiscient of the US Crime data, but different. 

Description

This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.

## Data 

```{r, echo = T}
states=row.names(USArrests)
states
```

## Variable Names {.smaller}

```{R, echo = T}
names(USArrests)
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
```

-  If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the Assault variable, since it has by far the largest mean and variance.


## PCA 

```{r, echo = T }
pr.out=prcomp(USArrests, scale.=TRUE)
names(pr.out)
pr.out$center
pr.out$scale
```


## Loadings 

The rotation matrix provides the principal component loadings; each column
of `pr.out$rotation` contains the corresponding principal component
loading vector.

```{r, echo = T}
pr.out$rotation
dim(pr.out$x)
```

## Biplots 

-  Biplot provides an efficient way of visualizing together objects (observations) and variables. 
-  The emphasis of the PCA of the city crime data was on cities
(objects). 
-  However, a graphical representation of the data that contains some information about the variables is particularly useful.

## Biplot {.smaller}

```{r, echo = T, fig.asp = 0.7}
biplot(pr.out, scale=0,  cex=c(0.6, 0.75),xlim=c(-4,4))
```

The scale=0 argument to biplot() ensures that the arrows are scaled to
represent the loadings.


<!-- ## Fancier version {.smaller} -->

<!-- ```{r, echo = T, message = F, fig.asp = 0.6} -->
<!-- # library(devtools); install_github("vqv/ggbiplot") -->
<!-- library(ggbiplot) -->
<!-- ggbiplot(pr.out, labels =  rownames(USArrests)) -->
<!-- ``` -->


## Biplot {.smaller}

**The principal components are only unique up to a sign change** 

```{r, echo = T, fig.height = 4}
pr.out$rotation=-pr.out$rotation; pr.out$x=-pr.out$x
biplot(pr.out, scale=0,cex=c(1/2, 1),xlim=c(-4,4))
```


## Importance of Scaling {.smaller} 

```{r, echo = T}
biplot(prcomp(USArrests, scale. = F), scale=0, cex=c(1/2, 1))
```


## Standard Deviations 

```{R, echo = T}
pr.out$sdev
pr.var=pr.out$sdev^2
pr.var
pve=pr.var/sum(pr.var)
pve
```

## Proportion of Variance Explained 

```{r, echo = F, fig.asp = 0.7}
par(mfrow=c(1,2))
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", ylim=c(0,1),type='b')
```


## More Examples 

-  Now we will see two more examples, using the princomp package. 
-  Recall that the `prcomp` and `princomp` package both performs PCA. One does SVD on the $X$ matrix and the other does Eigen-decomposition on the $X^TX$ matrix. 
-  They are **almost** identical althouch `prcomp` is preferred by some. 

## PCA for Crime Data-set

The following R commands perform PCA on the city crime dataset. The data give crime rates per 100,000 people for the 72 largest US cities in 1994. 

The variables are:
1) Murder
2) Rape
3) Robbery
4) Assault
5) Burglary
6) Larceny
7) Motor Vehicle Thefts

## The Data 

The data are in the "city_crime.txt" file that I read next.
```{r, echo = T}
setwd("~/GitHub/DattaHub.github.io/data-analytics/data")
crime<-read.table("city_crime.txt")
```

- This is city-wise crime data rather than state-wise like the last example. 


## Scatterplot Matrix

```{r, echo = T}
pairs(crime)
```

## R

Then apply PCA on the data by rescaling and centering the 
data using the R function `princomp`:

```{r, echo = T}
pca.crime<-princomp(scale(crime),cor=FALSE)
```

## Look at the results

```{r, echo = T}
summary(pca.crime)
```

## Loadings 

```{r, echo = T}
loadings(pca.crime)
```

## Proportion of Variance 

```{r, echo = T}
PoV <- pca.crime$sdev^2/sum(pca.crime$sdev^2)
barplot(PoV, main="Prop Var Exp")
```

## How many?

```{r, echo = T}
screeplot(pca.crime, type="line")
```

## Calculate the principal component scores

Once we calculate the principal components, we can plot them
against each other in order to produce low-dimensional views of the data.

First we calculate the principal component scores. 

```{r, echo = T}
pcs.crime<-predict(pca.crime)
```

## Plot the first 2 PCs. 

```{r, echo = T}
plot(pcs.crime[,1:2],type="n",xlab='1st PC',ylab='2nd PC') 
text(pcs.crime[,1:2],row.names(crime))
```


## Biplot {.smaller}


```{r, echo = T}
biplot(pca.crime, cex = c(0.5,1))
```


## Questions 

1.  Which cities are most safe? Which are most crime-prone?
2.  How to interpret high/low values in the second PC?

## Subtle difference {.smaller}

- For scaling, `prcomp` uses $n-1$ as denominator but `princomp` uses $n$ as its denominator. 

```{r, echo = T, warning = F, message=F}
pc.cr<-prcomp(crime, scale=TRUE, cor=TRUE, scores=TRUE)
pc.cr1<-princomp(crime, scale=TRUE, cor=TRUE, scores=TRUE)
pc.cr$scale
pc.cr1$scale
```


## Iris Data

- We consider the iris dataset again. 
- Here the geometric features of the iris flowers can be used to cluster them into different species: setosa, versicolor, virginica. 

```{r, echo = T}
data(iris)
str(iris)
dat.iris = data.matrix(iris[,1:4]) ## features except the species name. 
```

## Add noise and shuffle columns

-  We add 6 extra columns of random normal noise. These extra columns have no information about the species. 

-  We add the noise columns as columns 1-6, so now we have extra dimensions that we'd like to reduce. 

```{r, echo = TRUE}
set.seed(2023)
dat.iris = cbind(array(rnorm(150*6), c(150, 6)),dat.iris)
colnames(dat.iris)[5:10] <- paste('V', 1:6, sep='')
```

-  Now we have original variables as well as noise variables but we don't know which is which. 

- Let's see if PCA can still detect the species. 

## Center and Scale before PCA

```{r, echo = T}
dat.iris = scale(dat.iris, center=TRUE, scale=TRUE)
```

Perform principal component analysis on correlation matrix

```{r, echo = T}
pca.iris <- princomp(dat.iris, cor=TRUE)
```

## Summarize PCA {.smaller}

```{r, echo = T}
summary(pca.iris)
```

## Choose no. of PCs to look into

```{r, echo = T}
screeplot(pca.iris, type="line")
```

## Calculate PC scores {.smaller}

```{r, echo = T}
pcs.iris <- predict(pca.iris)
```

Set color code for observations from different groups

```{r, echo = TRUE}
col.code = as.character(iris$Species)
col.code[col.code == "setosa"] = 'red'
col.code[col.code == "versicolor"] = 'blue'
col.code[col.code == "virginica"] = 'green'
```


## Plot first two principal components {.smaller}

```{r, echo = TRUE, fig.height=5}
plot(pcs.iris[,1:2], type='p', pch=15, col=col.code, xlab="PC1", ylab="PC2")
legend("topright", col=c('red', 'blue', 'green')
     , legend = c('setosa', 'versicolor', 'virginica'), pch = 15
      )
```

## Importance of PC {.smaller}

-  Plot first two variables $X[,1:2]$ instead of the first two principal components. 
-  Do we see any clustering?

```{r, echo = T, fig.asp = 0.5}
plot(dat.iris[,1:2], type='p', pch=15, col=col.code, xlab="X1", ylab="X2")
legend("topright", col=c('red', 'blue', 'green'),legend = c('setosa', 'versicolor', 'virginica'), pch = 15)
```

## Importance of PC {.smaller}

**When we have many variables, projecting observations into top few PC scores help avoid searching over important variables.**

```{r, echo = T, fig.height=4}
plot(pcs.iris[,1:2], type='p', pch=15, col=col.code, xlab="PC1", ylab="PC2")
legend("bottomleft", col=c('red', 'blue', 'green'), legend = c('setosa', 'versicolor', 'virginica')
     , pch = 15
      )
```

# Principal components regression 

## Fix notations

Each of the eigenvectors $\phi_j$ is called a **principal component**.

It is basically finding the principal axes on which the data vary.

- Axes are given by $\phi_j$,
- and variability by $d_j^2$.


## Principal components regression

The most common application of principal components is in creating a low-dimensional set of orthogonal predictors.

We presume that the $\phi_j$ have been ordered by decreasing $d_j^2$,

- so that $z_1 = X \phi_1$ has the largest sample variance amongst all normalized linear combinations of the columns of $X$.

**Principal components regression (PCR)**

- forms the derived input columns $z_j = X \phi_j$,
- and the regresses $y$ on $z_1,\dots, z_m$ for some $m \ll p$.  

## Sum of univariate regressions

On you homework you will show that since the $z_j$'s are orthogonal, the regression is just a sum of univariate regressions.  

Ignoring the intercept,

$$
\hat{y}^{\mathrm{pcr}} = \sum_{j=1}^m \hat{\theta}_j z_j,
\quad \mbox{ where } \quad \hat{\theta}_j  = z_j^\top y / z_j^\top z_j
$$

- although $\hat{\theta} = (Z_{(m)}^\top Z_{(m)})^{-1} Z_{(m)}^\top y$ works too (its just more computationally expensive).

The thinking is that $y$ will tend to vary most in the directions of high variance inputs $z_1, z_2, \dots$, which may or may not be true.

- We can do `step`-wise variable selection, 
- and the greedy solution will be optimal (in a sense, for those PCs).

## Backing out the coefficients for $X$

Since the $z_j$ are each a linear combination of the original $x_j$, we can express the solution $\hat{\theta}$ in terms of the original (scaled) $x_j$:

$$
\hat{\beta}^{\mathrm{pcr}}(m) = \sum_{j=1}^m \hat{\theta}_j v_j.
$$

Notice that even though the sum only goes to $m$, 

- implying that $(d-m)$ of the $\theta_j$-coefficients for $j > m$ are effectively zero,
- all of the $\hat{\beta}^{\mathrm{pcr}}_j$ coefficients, $j=1,\dots,p$, are nonzero.

## Toy PCR example

Consider data on 

- scores of six attributes from a sensory panel (potential $y$-values) 
- as predicted by measurements of five psysico-chemical quality parameters
- on $n=16$ olive oil samples (5 Greek, 5 Italian, 6 Spanish).

Read in the data, focusing on the first attribute (as our $y$-variable), and standardize the $X$'s.

```{r, message=FALSE}
library(pls)
data(oliveoil)
y <- as.numeric(oliveoil$sensory[,1])
X <- as.matrix(oliveoil$chemical)
for(i in 1:ncol(X)) { X[,i] <- X[,i] / sd(X[,i]) }
```

## PCA calculations

First the Eigen-decomposition

```{r}
S <- cov(X)
eS <- eigen(S, symmetric=TRUE)
```

Then project onto the PC-space

```{r}
Z <- X %*% eS$vectors
colnames(Z) <- paste("PC", 1:5, sep="")
cumsum(eS$values)/sum(eS$values)
```

- The first three PCs are probably sufficient.

## PCA extensions

Using only the first three PCs, add an intercept and calculate $\hat{\theta}$.

```{r}
Zp <- cbind(1, cbind(Z[,1:3]))
theta.pcr <- drop(solve(t(Zp) %*% Zp) %*% t(Zp) %*% y)
theta.pcr
```

Now map $\hat{\theta}$ back into $\beta$ and $X$ space.

```{r}
beta.pcr <- matrix(c(theta.pcr[1], eS$vectors[,1:3] %*% theta.pcr[-1]), nrow=1)
colnames(beta.pcr) <- c("Intercept", colnames(oliveoil$chemical))
beta.pcr
```

## Library routine

The `pls` package provides a routine automating the process.

```{r}
fit.pcr <- pcr(y ~ X, ncomp=3)
rbind(automated=coef(fit.pcr, intercept=TRUE), byhand=as.numeric(beta.pcr))
```

`pls` will also do CV for you.

```{r}
cv.pcr <- pcr(y ~ X, validation="LOO")
RMSEP(cv.pcr)
```

## CV visual

Perhaps we can get away with as few as one or two PCs.

```{r, dev.args=list(bg='transparent'), fig.width=5, fig.height=3.75, fig.align="center", no.main=TRUE}
plot(RMSEP(cv.pcr), main="")
```

