---
title: Sign test
subtitle: Formula and Codes
author: Jyotishka Datta
fontsize: 11pt
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---
class: inverse, center, middle

## Nonparametric tests

"... which is what you do when you don't believe the assumptions for the classical tests we have learned so far." 

[Charlie Geyer]


---
## Nonparametric tests 

-   What happens when we have a non-Gaussian sample?  
-   **Remember**: The sampling distribution of the mean (or the difference between means for the independent samples t-test) should be approximately normally distributed. 
-   This assumption is not about the raw data being normally distributed, but rather about the means being so. 
-  Given the Central Limit Theorem, this assumption can be relaxed when the sample size is large.


---
## Example: Bimodal distributions

.pull-left[

- Generate samples: 60% from N(3, 1) and 40% from N(7, 1)

```{r}
set.seed(123) # Set seed for reproducibility
n <- 1000  # number of samples
samples <- c(rnorm(0.6 * n, 3,1), 
             rnorm(0.4 * n, 7,1))
```
]


.pull-right[
```{R}
hist(samples)
```
]

---
## Sign test 

- **Idea**: median divides any sample distribution into two equal halves. 

-  If the true median is $\mu_0$, then approximately half the samples would exceed $\mu_0$ and half will fall below it. 

- This is true irrespective of the distribution of your raw data or your sampling distribution. 


```{r, echo = F, warning = F, fig.align = 'center', fig.height=4}
# Load necessary libraries
library(ggplot2)
# Calculate median
med <- median(samples)

# Assign colors based on the relation with the median
colors <- ifelse(samples <= med, "blue", "red")

# Plot histogram with ggplot2
ggplot(data.frame(samples, colors), aes(x = samples, fill = colors)) +
  geom_vline(xintercept = med) +
  geom_vline(xintercept = mean(samples), linetype = "dashed") +
  geom_histogram(binwidth = 0.5, position = "identity", alpha = 0.75) +
  labs(title = "Solid line is the median, dashed line is the mean",
       x = "Value",
       y = "Frequency") +
  theme_minimal()
```

---
## Mean > Median 

- If you check what proportion of observations fall below the median or mean, you will roughly see that median divides into two equal halves but mean does not. 

```{r}
samples <- c(rnorm(0.6 * n, 3,1), 
             rnorm(0.4 * n, 7,1))
(mean((samples <= median(samples))))
(mean((samples <= mean(samples))))
```
---
## Right skewed 

- Similar phenomenon for samples from a right-skewed distribution, e.g. exponential distribution with $\lambda = 5$. 

```{r}
samples <- rexp(1000, rate = 5)
(mean((samples <= median(samples))))
(mean((samples <= mean(samples))))
```

```{r, echo = F, warning = F, fig.align = 'center', fig.height=3}
# Calculate median
med <- median(samples)
# Assign colors based on the relation with the median
colors <- ifelse(samples <= med, "blue", "red")

# Plot histogram with ggplot2
ggplot(data.frame(samples, colors), aes(x = samples, fill = colors)) +
  geom_vline(xintercept = med) +
  geom_vline(xintercept = mean(samples), linetype = "dashed") +
  geom_histogram(bins = 50, position = "identity", alpha = 0.75) +
  labs(title = "Solid line is the median, dashed line is the mean",
       x = "Value",
       y = "Frequency") +
  theme_minimal()
```

---
## Recap: Binomial Distribution

-  Represents a sequence of independent coin tossing experiment. 
-  Suppose a coin with probability $p$; $0<p <1$ for heads in a single trial is tossed independently a pre-specified $n$ times, $n \geq 1$. 
-  Let $X$ be the number of times in the $n$ tosses that a head is obtained. Then the pmf of $X$ is:

$$
P(X = x) = {n \choose x} p^x (1-p)^{n-x}, x = 0,1,\ldots, n
$$

-  Binomial is sum of $n$ independent Bernoulli trials. 

- If $X \sim \text{Bin}(n,p)$. Mean $E(X) = np$, Variance: $V(X) = np(1-p)$. 

---
## Sign-test 

-   **Data**: $x_1, \ldots, x_n \stackrel{IID}{\sim} F$ with median $M$.

-   **Null hypothesis:** $M = \mu_0$ and **Alternative:** $M \ne \mu_0$ or $M > \mu_0$ or $M < \mu_0$.

-   Test statistic: $S$ = the number of observations that exceed $\mu_0$.

-   Strategy: Reject the null if $S$ is too big or too small (depending on the alternative).

-   If $H_0$: $M = \mu_0$ is true we would expect 50% of the observations to be above $\mu_0$, and 50% of the observations to be below $\mu_0$.

-   Observe: $S = \sum_{i=1}^{n} 1(X_i > \mu_0)$, and $P(X_i > \mu_0 \mid H_0) = 1/2$ for each $i$. 

-  Question: What is the distribution of each $X_i$, and $S$?                         
---
## Sign test 

- Binomial is sum of $n$ independent Bernoulli trials. 
-  $X_i \sim \text{Bernoulli}(p)$ for $i = 1, \ldots, n$, then $X = \sum_{i=1}^{n} X_i \sim \text{Bin}(n, p)$.

>- The sign-test statistic $S$ has a binomial distribution

>- $S \sim \text{Bin}(n,p)$, where $p$  = the probability that an observation is greater than $\mu_0$. 

--

-  In particular, if $H_0$ is true then $S$ will have a binomial distribution: $S \sim \text{Bin}(n, 1/2)$.

-  If $X \sim \text{Bin}(n,p)$. Mean $E(X) = np$, Variance: $V(X) = np(1-p)$. 

- What are the mean and variance of $S$ under $H_0$? 

--

-  Under $H_0: p = 1/2$, $E(S) = \frac{n}{2}$ and $V(S) = \frac{n}{4}$. 
-  Intuitively, if $S$ is too far away from $n/2$, $H_0$ must be rejected. 
---
## Sign test 

-   Hypothesized median = $\mu_0$.
-   Test statistics: $S$ = number of observations greater than $\mu_0$. 
-   Observed value of $S$ = $S_{obs}$.
-   Compute the P-value:

$$
\begin{equation}
	\text{P-value} = \begin{cases}
	P(S \ge S_{obs}) \; \text{ if } H_A: \mu > \mu_0 \text{ or } \mu < \mu_0\\
	2 P(S \ge S_{obs}) \; \text{ if } H_A: \mu \neq \mu_0
	\end{cases}
\end{equation}
$$

-   Under the null: $H_0: \mu = \mu_0$: $S \sim \text{Bin}(n, 1/2)$.
-   We can use exact binomial probability calculations using CDF of a $\text{Bin}(n, 1/2)$ distribution, i.e. a `pbinom` function. 
-   We can use R, there is `binom.test` function. 

---
## Cholesterol data

```{r}
cholesterol <- read.csv("cholesterol.csv")
str(cholesterol)
(S <- sum(cholesterol$Reduction>0)) ## test stat
```
-  The test statistics is $S =$ `r S`. 

-  We need to find out what is the probability that a $Bin(10, 0.5)$ random variable takes a value higher than `r S`. s

---
## P-value 

- Once again, we use the `pbinom` for calculating the tail probabilities. 

```{r}
S <- sum(cholesterol$Reduction>=0)
n <- length(cholesterol$Reduction)

(pval <- (1-pbinom(S-1, size = n, prob= 0.5)))

```

- This is larger than our usual 5% threshold. 

---
## R function 

- As we showed in class, `R` also has a function `binom.test()` 

```{r}
S <- sum(cholesterol$Reduction>=0)
n <- length(cholesterol$Reduction)
binom.test(S, n, alternative = "greater")
```

---
## CLT 

-  We can use CLT to obtain a large-sample approximation. 

-  There is a caveat: CLT kicks in after a certain sample size, so if your sample size is too small, then this approximation could be off. 

![DeMoivre-Laplace CLT](clt-1.png)

---
## CLT for sign test 

-  The demoivre-Laplace CLT tells us that if $X \sim \text{Bin}(n, p)$, then we can approximate the $\le$-type probability $P(X \le k)$ as:
$$
\begin{align}
P(X \le k) & = P \left( \frac{X- np}{\sqrt{np(1-p)}} \le \frac{k- np}{\sqrt{np(1-p)}} \right) \\
 & \approx \Phi\left(\frac{k- np}{\sqrt{np(1-p)}} \right)
\end{align}
$$


-  In DeMoivre-Laplace CLT, we are using a continuous distribution to approximate a discrete distribution taking only integer values.

-  The quality of the approximation improves, sometimes dramatically, if we fill up the gaps between the successive integers. 

---
## CLT for sign test 

-  To approximate $P(X \le k)$ we expand the domain of the event to $k + 1/2$ and approximate $P(X \le k)$ as:
$$
P(X \le k) \approx \Phi\left(\frac{k + 1/2- np}{\sqrt{np(1-p)}} \right)
$$


-  This adjusted normal approximation is called **a normal approximation with a continuity correction**. 

---
## CLT for sign test 

-  Sign test uses a test-statistics $S \sim \text{Bin}(n, 1/2)$ under the null hypothesis. 

-  Suppose the alternative is one-sided: $H_A: \mu > \mu_0$. 

-  If $n \ge 12$, the P-value can be approximated as: 

$$
\begin{equation}
p = 1 - \Phi(\frac{S_{obs}-0.5 - 0.5n}{\sqrt{0.25n}}) 
\end{equation}
$$


-  Therefor for $n \ge 12$, the rejection rule would be:

-  Reject $H_0$ if $S_{obs} \ge 0.5n + 0.5 + z_{\alpha}\times \sqrt{0.25 n}$, where $z_{\alpha}$ is the $\alpha^{th}$ percentile of $N(0,1)$, i.e. 

$$
\begin{equation}
P(Z \ge z_{\alpha}) = \alpha, \; z_{0.025} = 1.96, \; z_{0.05} = 1.64. 
\end{equation}
$$
---
## Try for cholesterol data 

Now if we use CLT to calculate the P-value:

First, p-value = $P_{S \sim \text{Bin}(10,1/2)} (S \ge 8)$.

Applying CLT formula:
$$
\begin{align}
p_{CLT} & = 1 - \Phi\left(\frac{S_{obs}-0.5 - 0.5n}{\sqrt{0.25 n}} \right) \\
& = 1 - \Phi\left(\frac{8-0.5 - 5}{\sqrt{0.25\times 10}} \right) = 1 - \Phi(1.58) = 0.0569.
\end{align}
$$

-  Compare this approximate P-value with the original (p = 0.0547). 

-  Quite close even though $n$ is only 10. 

```{r}
Z <- (S-0.5-0.5*n)/(sqrt(0.25*n))
(p <- 1 - pnorm(Z))
```
---
## Simulation based p-value 

- We can also use simulation to calculate the P-value.

- We can simulate a **large** number of samples from the null distribution of the test statistics and see what proportion of observations falls beyond the observed test statistics. 

- The null distribution of $S$ is $\text{Binomial}(n = 10, 1/2)$. 

```{R}
set.seed(12345)
null.samples <- rbinom(10000, size = 10, 1/2)
(pval.mc <- mean((null.samples>=S)))
```
- You can see that it's quite close to the original P-value. 



