---
title: "Chi-square tests"
author: "Jyotishka Datta"
date: "Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
  css: mystyle.css
lib_dir: libs
nature:
  highlightStyle: github
highlightLines: true
countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

## Goals for today 


-  We will first talk about the $\chi^2$ distribution and some of its properties here. 

-  Then we will talk about two different tests or two uses. 

-  The first is for testing hypothesis about the population variance $\sigma^2$. 

-  The second is our focus today: for testing goodness of fit. 

---

## Chi-sqaure distribution 

- The chi-squared distribution (also chi-square or $\chi^2$) with $k$ degrees of freedom is the distribution of a sum of the squares of $k$ independent standard normal random variables.

- As usual, R has four helper functions:
- `dchisq`: for density, `pchisq`: CDF, `qchisq`: quantile and `rchisq`: random numbers. 

---
## Sample generation 

- We can use `rchisq` to generate samples from the $\chi^2$ distribution. The only parameter we will specify is the `df`: degrees of freedom. 

```{r, fig.asp = 0.6}
chisq.sim <- rchisq(1000, df = 5)
hist(chisq.sim, freq = F)
curve(dchisq(x,df=5), add = T)
```

---
## $\chi^2_k$ is sum of squares of $k$ standard normals

```{r, fig.asp = 0.5}
chisq.sim = NULL
for(i in 1:1000){
  x <- rnorm(5)
  chisq.sim = c(chisq.sim, sum(x*x))
}
hist(chisq.sim, freq = F)
curve(dchisq(x, df = 5), add = T)
```


---
## Implication

-  This sum of squares of normals has an implication. 

-  First, if $Y \sim N(0,1)$, $Y^2 \sim \chi^2_{1}$
-  Equivalently,  $Y \sim N(\mu, \sigma)$, $((Y-\mu)/\sigma)^2 \sim \chi^2_{1}$

-  Second, if $Y_1, Y_2, \ldots, Y_n \sim N(0,1)$, 
we can say $\sum_{i=1}^{n} Y_i^2 \sim \chi^2_n$. 

---
## Chi-square test for sample variance

- Recall that our unbiased estimator for $\sigma^2$ is:
$$
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (y_i - \bar{y})^2
$$

-  It turns out that the distribution of $(n-1)s^2/\sigma^2$ follows a $\chi^2$ distribution with $(n-1)$ degrees of freedom. 

- The proof is easy. 

---
## Proof 

$$
\begin{align}
Y_i & \sim N(\mu, \sigma^2), \; i = 1, 2, \ldots, n \\
\Rightarrow & \sum_{i=1}^{n}((Y_i-\mu)/\sigma)^2 \sim \chi^2_{n} \\
\Rightarrow \frac{1}{\sigma^2}&\left[ \sum_{i=1}^{n}\{(Y_i-\bar{Y})^2 + (\bar{Y}-\mu)^2 \}\right] \sim \chi^2_{n} \\
\Rightarrow  \frac{1}{\sigma^2} &\left[ (n-1) s^2 + n(\bar{Y}-\mu)^2 \}\right] \sim \chi^2_{n} \\
\Rightarrow & \frac{(n-1)s^2}{\sigma^2} + \underbrace{\left\{ \frac{(\bar{Y}-\mu)}{\sigma/\sqrt{n}} \right\}^2}_{\chi^2_1} \sim \chi^2_{n} \\
\Rightarrow & \frac{(n-1)s^2}{\sigma^2} \sim \chi^2_{n-1}
\end{align}
$$
---
## We can use for testing 

-  Show with simulated data from Normal distribution with $\sigma = 2$. 

-  To test generate samples from $\chi^2_{n-1}$, get candidate $s^2$ values by $s^2 \sim \sigma^2/(n-1) \chi^2_{n-1}$.

-  Plot generated data and observed $\bar{y}$. 

```{r, eval = F}
set.seed(12345)
## one sample versus mu=0
y <- rnorm(20, mean = 0, sd = 2)
ybar <- mean(y)
s2 <- var(y)
n <- length(y)
mu <- 0
N <- 100000
sigma2 <- (n-1)*s2 / rchisq(N, n-1)
ybars <- rnorm(N, mu, sqrt(sigma2/n))
hist(ybars)
abline(v=ybar)
```


---
## We can use for testing 

```{r, eval = T, echo = T, fig.asp = 0.5}
set.seed(12345)
## one sample versus mu=0
y <- rnorm(20, mean = 0, sd = 2)
ybar <- mean(y)
s2 <- var(y)
n <- length(y)
mu <- 0
N <- 100000
sigma2 <- (n-1)*s2 / rchisq(N, n-1)
ybars <- rnorm(N, mu, sqrt(sigma2/n))
hist(ybars)
abline(v=ybar)
```
---
class: inverse, center, middle

## Goodness-of-fit tests

---
## Goodness of fit 

-  The chi-square goodness of fit tests have a different purpose! 

-  They are used for testing whether a discrete data fits a probability distribution! 

-  But, they stem from the idea that for a frequency (or contingency) table, with number of observations of type $i$ being $O_i$ and expected frequency being $E_i$, 

- $\sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i} \sim \chi^2_{k-1}$

-  This is an 'asymptotic' result and it follows from a careful application of CLT. First proved by Karl Pearson. 


---
## Goodness of fit tests in R 

- This function is used for both the goodness of fit test and the test of independence, depending upon what kind of data you feed it. 

```{r, echo = T, eval = F}
chisq.test(x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)), 
           rescale.p = FALSE)
```

-  If "x" is a numeric vector, a goodness of fit test will be done (or attempted), treating "x" as a vector of observed frequencies. 
-  If "x" is a 2-D array, or matrix, then it is assumed to be a contingency table of frequencies, and a test of independence will be done. (we shall learn this later!)

---
## `chisq.test` arguments 

```{r, echo = T, eval = F}
chisq.test(x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)), 
           rescale.p = FALSE)
```

-  `correct`: continuity correction. 
-  `p`: a vector of probabilities of the same length of x, default is 1/length(x). 
-  `rescale.p` : The candidate distribution needs to be a valid probability mass function such that the sum of $p_i$'s is equal to 1. 
-   If you don't have the distribution normalized set `rescale.p` to `TRUE`.


---
## Example 1 

Professor A is teaching Stat 101, and the distribution of students in his classroom are as follows: There are 10 freshman in the sample, 15 sophomores, 22 juniors, and 28 seniors. Test the null hypothesis that freshman, sophomores, juniors, and seniors are equally represented among students signed up for Stat 101.

The frequencies are $x = (10,15,22,28)$, and we want to test if

$$
\begin{gather*}
H_0: \; p_1 = p_2 = p_3 = p_4 = 1/4 \\
\text{ vs. }  \\
H_1: \; \text{not all } p_i's \text{ are equal}
\end{gather*}
$$

---
## R command 

```{r}
chisq.test(c(10,15,22,28))
```

-  We don't need to specify the p vector, since by default the null is equal frequencies. 
-  Chi-square statistics= 9.96. P-value = 0.01891 < 0.05, Reject null.
-  What is the degrees of freedom? 


---
## Example 1 (continued)

Now suppose Professor B thinks that the number of freshman and sophomores enrolled is each half the number of juniors and the number of seniors, i.e. proportions are: $p_0 = (1/6, 1/6, 1/3, 1/3)$.

$$
\begin{gather*}
H_0: \; p_1 = p_2 = 1/6, p_3 = p_4 = 1/3 \\
\text{ vs. } \\
H_1: \; \text{not all } p_i s \text{ are equal to the corresponding } p_{0i}s.
\end{gather*}
$$

---
## R command 

-  You need one more argument `p` to specify the non-uniform probabilities. 

```{r}
null.probs = c(1/6,1/6,1/3,1/3)
freqs = c(10,15,22,28)
chisq.test(freqs, p=null.probs) ## must label p, it's not the second option. 
```

-  Chi-square statistics= 1.72. P-value: 0.6325. (Do not reject null). 
-  **Warning**: since R doesn't expect "p" as the 2nd argument, you must explicitly mention `p = ...`.

---
class: inverse, center, middle

## Two-samples goodness-of-fit tests

---
## Two samples 

-  We shall look at two samples more carefully later, but it does not hurt to mention at this point. 
-  Data: $X_1, X_2, \ldots, X_n \sim F_1$, and $Y_1, Y_2, \ldots, Y_n \sim F_2$. 
-  Test: $H_0: F_1 = F_2$ vs. $H_1: F_1 \neq F_2$. 
-  Apply the same procedure, treat the proportions from one sample as the 'true' null proportions. 


---

## Example in R 

```{r}
new_freq = c(10,15,22,28)
old_freq <- c(20,16,18,9)
chisq.test(new_freq, p=old_freq/sum(old_freq))
```

---
## Use `rescale.p` option

```{r}
new_freq = c(10,15,22,28)
old_freq <- c(20,16,18,9)
chisq.test(new_freq, p=old_freq, rescale.p = TRUE)
```


---
## Chi-square test for Continuous data 

-  Chi-square test can be adapted for continuous data if we create our own categories. 

-  I will show you one example for the sake of completeness but for the sake of this class, we will use chi-square GoF test for discrete data and K-S and Lilliefors test for continuous data.

---
## Testing equality of two distributions 

-  Samples from a Gamma distribution. 

```{r, fig.height = 4}
num_of_samples = 1000
x <- rgamma(num_of_samples, shape = 10, scale = 3)
hist(x, breaks = 50, col = rgb(0,0,1,0.5))
```

---
## Now we add jitters 

-  Small white noise to the data !

```{R, fig.height = 4}
hist(x, breaks = 50, col = rgb(0,0,1,0.5))
x <- x + rnorm(length(x), mean=0, sd = .1) # add jitter
p1 <- hist(x, breaks = 50, col = rgb(1,0,1,0.5), add = T)
```

---
## How to apply Chi-square? 

-  We can use the bins of the histogram as the categories.

-  The probability of $X \sim \text{Gamma}(shape = \alpha, scale = \lambda)$ lying within a bin $(a,b]$ would be:

$$
\begin{align*}
P( X \in (a,b]) & = F(b) - F(a), \\
\Rightarrow P( X \in (a,b]) & =  pgamma(b, \alpha, \lambda) - pgamma(a, \alpha, \lambda)
\end{align*}
$$
- So we need the boundaries of the bins, and apply the cdf `pgamma` to them. 

---
## This is how it's done 

```{r, message = FALSE, warning = FALSE}
## uncomment line 1 before running for the first time
## install.packages("zoo")
library('zoo')
breaks_cdf <- pgamma(p1$breaks, shape=10, scale=3)
(null.probs <- rollapply(breaks_cdf, 2, function(x) x[2]-x[1]))
```


---
## Barplot from a histogram? 

```{r, fig.height=3}
barplot(null.probs)
```

-  Basically, we have created a discrete pmf from a pdf for using chi-square test. 

---

## Now test 

```{r}
chisq.test(p1$counts, p=null.probs, rescale.p=TRUE)
```


---
## Monte Carlo 

-  The chi-square test needs to be run using Monte Carlo to make sure its result is accurate enough. For use the Monte Carlo set `simulate.p.value` to `TRUE`. 
-  You can also set the iteration number by set B.

```{r}
chisq.test(p1$counts, p=null.probs, rescale.p=TRUE, simulate.p.value=TRUE)
```


---
## Kolmogorov-Smirnov Test 

- As I said a few slides back, the chi-square tests are not ideal for testing equality of distributions for continuous data and there is a much better alternative called the Kolmogorov--Smirnov test. 

-   Much simpler since there's no binning, only the absolute difference of two CDFs. 

```{r}
x <- rgamma(num_of_samples, shape = 10, scale = 3)
y <- x + rnorm(length(x), mean=0, sd = .1) # add jitter
ks.test(x,y)
```

-  We will learn about this later! 
