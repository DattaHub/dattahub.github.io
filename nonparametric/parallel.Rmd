---
title: "Introduction to parallel computing"
author: "Jyotishka Datta"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
  css: mystyle.css
lib_dir: libs
nature:
  highlightStyle: github
highlightLines: true
countIncrementalSlides: false

---
class: center, middle, inverse
count: false

# Parallel 

---
## A gentle start to parallel computing 

- Parallel processing breaks up your task, splits it among multiple processors, and puts the components back together.
- Useful and easy if you have a task that can be split up, especially without the different parts needing to "talk to" each other.
- On your typical computer, implementing parallel processing in R might speed up your program by a factor of 2 to 4.
- If you have access to clusters on the web, many many "cores".

---
## How do I know if I can apply these ideas? 

- Parallel processing works best when you have a task that has to be completed many times, but each repeat is independent.
- For instance, if you are repreating a simulation, and in each case are drawing new parameters from a distribution, that is an easily parallelizable task.
- A good rule of thumb is that if you can wrap your task in an apply function or one of its variants, it's a good option for parallelization. 
- In fact most implementations of parallel processing in R are versions of apply.


---
## What happens?

- When you run a task in parallel, your computer "dispatches" each task to a CPU core. 
- This dispatching adds computational overhead. 
- So, it's usually best to try to minimize the number of dispatches. 
- In most cases you are going to have a small number of computing cores relative to your tasks. A powerful laptop or desktop will have 2, 4, or 8 cores, and even the most powerful Amazon virtual machine has 88.

---
## Introduction to doParallel

- We will first write a super simple R code to explain the mechanism of parallel processing using `doParallel`. 

- We use `Sys.sleep()` function that puts the system to sleep for a specified time but run it parallely. If we're doing this in parallel with more than or equal to 2 clusters the system should take around 1 second, not 2. 

```{r, echo = TRUE, eval = FALSE, warning=FALSE, message = FALSE, cache = TRUE}
n = as.numeric(Sys.getenv("NUMBER_OF_PROCESSORS")) # on Windows
require(doParallel)
cl <- makeCluster(n)
registerDoParallel(cl)
ptm <- proc.time()
foreach(i=1:2) %dopar% {Sys.sleep(1)}
proc.time()-ptm
```

---
## Output of the snippet 

- Here is the output! (The list has four `NULL` values because the function is not returning anything, it's just waiting for 1 second at each processor.)

```{r, echo = TRUE, eval = TRUE, warning=FALSE, message = FALSE}
n = as.numeric(Sys.getenv("NUMBER_OF_PROCESSORS")) # on Windows
require(doParallel)
cl <- makeCluster(n)
registerDoParallel(cl)
ptm <- proc.time()
foreach(i=1:2) %dopar% {Sys.sleep(1)}
proc.time()-ptm
```


---
## Sequential counterpart 

- This should take 2 seconds as no parallelism is being done. 

```{r, echo = TRUE}
z=vector('list',2)
z=1:2
#Four pauses of one second each.
system.time(lapply(z, function(x) Sys.sleep(1)))
```

---
## Hello, world !

- This is our "Hello, world" program for parallel computing. 
- It tests that everything is installed and set up properly, **but don't expect it to run faster than a sequential for loop, because it won't**! 
-  With small tasks, the overhead of scheduling the task and returning the result
can be greater than the time to execute the task itself, resulting in poor performance. 


---
## We will see a non-trivial example 

- Non-trivial examples need sophisticated Statistical methods, and I haven't covered these yet. So don't worry at all if you don't understand any of it. I just want to show you an example of the advantage of using parallel processing. 

- How long does it take to do 10,000 Bootstrap iterations using 2 cores?


---
## Bootstrapping GLM 

```{r}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
str(x)
```

- Logistic Regression: predict species using sepal.length.
- Want the distribution of parameter estimates.
- Fit the model repeatedly (like 10k times) for observations randomly sampled with replacements.
- Gives us Bootstrap distribution. 
- Cover Bootstrap in greater details towards the end. 

---
## Bootstrap using parallel 

```{r, echo = TRUE, cache = TRUE}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- 10000
ptime <- system.time({
r <- foreach(icount(trials), .combine=cbind) %dopar% {
  ind <- sample(100, 100, replace=TRUE)
  result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
  coefficients(result1)
}
})
ptime
```


---
## Sequential counter-part 

```{r, echo = TRUE, cache = TRUE}
stime <- system.time({
r <- foreach(icount(trials), .combine=cbind) %do% {
  ind <- sample(100, 100, replace=TRUE)
  result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
  coefficients(result1)
}
})
stime
```





